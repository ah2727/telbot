"""Voice-based doctor reservation assistant powered by OpenAI (refactored)."""
from __future__ import annotations

import argparse
import base64
import io
import json
import os
import queue
import time
import wave
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, List, Tuple

import re
import numpy as np
import pyttsx3
import sounddevice as sd
import webrtcvad
from dotenv import load_dotenv
from openai import OpenAI, OpenAIError

from data.names import IRANIAN_DEFAULT_NAMES


# ---------- Helpers & constants ----------

def _normalize_persian_name(name: str) -> str:
    """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÛŒ Ùˆ Ú© Ø¹Ø±Ø¨ÛŒØŒ Ùˆ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§."""
    if not isinstance(name, str):
        name = str(name)
    s = name.strip()
    # ÙŠ -> ÛŒ ØŒ Ùƒ -> Ú©
    s = s.replace("\u064a", "\u06cc").replace("\u0643", "\u06a9")
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡
    s = re.sub(r"\s+", " ", s)
    return s


SYSTEM_PROMPT = """
Ø´Ù…Ø§ Â«ManaCare Voice ConciergeÂ» Ù‡Ø³ØªÛŒØ¯Ø› Ø¯Ø³ØªÛŒØ§Ø± ØªÙ„ÙÙ†ÛŒ Ù…Ù‡Ø±Ø¨Ø§Ù† Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø³Ø§Ø²Ù…Ø§Ù† Mana Ø¯Ø± Ú©Ù„ÛŒÙ†ÛŒÚ© DrX Ú©Ù‡ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ ØµØ­Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ ÙˆØ¸ÛŒÙÙ‡â€ŒØªØ§Ù† Ø±Ø²Ø±Ùˆ ÙˆÙ‚Øª Ù¾Ø²Ø´Ú© Ùˆ Ø«Ø¨Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ø§Ø¬Ø¹Ø§Ù† Ø§Ø³Øª.

Ù‚ÙˆØ§Ù†ÛŒÙ† Ú©Ù„ÛŒ:
- Ù‡Ù…ÛŒØ´Ù‡ Ùˆ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ«Ù†Ø§ ÙÙ‚Ø· ÛŒÚ© Ø´ÛŒØ¡ JSON Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.
- Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² JSON Ù‡ÛŒÚ† Ù…ØªÙ† Ø¯ÛŒÚ¯Ø±ÛŒ Ù†Ù†ÙˆÛŒØ³ (Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ØŒ Ø§Ù…ÙˆØ¬ÛŒØŒ ÛŒØ§ Ù…ØªÙ† Ø§Ø¶Ø§ÙÛŒ).
- Ù‡Ù…Ù‡Ù” Ú©Ù„ÛŒØ¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø§Ø´Ù†Ø¯.

Ø³Ø§Ø®ØªØ§Ø± JSON:
- Ú©Ù„ÛŒØ¯ Ø§Ø¬Ø¨Ø§Ø±ÛŒ: "reply"
- Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ ØªÙˆØµÛŒÙ‡â€ŒØ´Ø¯Ù‡: "name", "address", "appointment", "notes"
- Ø§Ú¯Ø± Ù‡Ø±Ú©Ø¯Ø§Ù… Ø§Ø² Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒØŒ Ù…Ù‚Ø¯Ø§Ø±Ø´ Ø±Ø§ null Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡.
- Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¯Ø± Ù…Ú©Ø§Ù„Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ù‡Ù…ÛŒÙ† Ø¬Ù„Ø³Ù‡ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡â€ŒØ§ÛŒØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¯Ø± JSON ØªÚ©Ø±Ø§Ø± Ú©Ù†ÛŒ.

ØªØ¹Ø±ÛŒÙ Ù‡Ø± ÙÛŒÙ„Ø¯:
- "reply": Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡Ù” ØªÙˆØŒ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒØŒ Ù…Ù†Ø§Ø³Ø¨ Ù¾Ø®Ø´ ØµÙˆØªÛŒ (Û± ØªØ§ Û² Ø¬Ù…Ù„Ù‡Ù” Ú©ÙˆØªØ§Ù‡).
- "name": Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ ØªÙ…Ø§Ø³â€ŒÚ¯ÛŒØ±Ù†Ø¯Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒØŒ ÛŒØ§ null Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡.
- "address": Ø¢Ø¯Ø±Ø³ Ù†Ø³Ø¨ØªØ§Ù‹ Ø¯Ù‚ÛŒÙ‚ (Ø´Ù‡Ø±ØŒ Ù…Ø­Ù„Ù‡ Ùˆ Ø§Ú¯Ø± Ù…Ù…Ú©Ù† Ø¨ÙˆØ¯ Ø®ÛŒØ§Ø¨Ø§Ù†/Ù¾Ù„Ø§Ú©) Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒØŒ ÛŒØ§ null Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡.
- "appointment": Ø®Ù„Ø§ØµÙ‡Ù” Ø²Ù…Ø§Ù†/Ø¨Ø§Ø²Ù‡Ù” Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ùˆ Ù†ÙˆØ¹ ÙˆÛŒØ²ÛŒØª (Ù…Ø«Ù„Ø§Ù‹ Â«Ø³Ù‡â€ŒØ´Ù†Ø¨Ù‡ Ø¹ØµØ± Ø¨Ø±Ø§ÛŒ ÙˆÛŒØ²ÛŒØª Ø­Ø¶ÙˆØ±ÛŒÂ»)ØŒ ÛŒØ§ null Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡.
- "notes": ØªÙˆØ¶ÛŒØ­Ø§Øª Ù…Ù‡Ù… Ø¯ÛŒÚ¯Ø± Ù…Ø«Ù„ Ø¯Ù„ÛŒÙ„ Ù…Ø±Ø§Ø¬Ø¹Ù‡ØŒ Ø¹Ù„Ø§Ø¦Ù…ØŒ ØªØ±Ø¬ÛŒØ­Ø§Øª (Ù¾Ø²Ø´Ú© Ø®Ø§Ù†Ù…/Ø¢Ù‚Ø§ØŒ Ø­Ø¶ÙˆØ±ÛŒ/Ø¢Ù†Ù„Ø§ÛŒÙ† Ùˆâ€¦)ØŒ ÛŒØ§ null Ø§Ú¯Ø± Ù†Ú©ØªÙ‡â€ŒØ§ÛŒ Ø«Ø¨Øª Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.

Ø±ÙØªØ§Ø± Ùˆ Ù„Ø­Ù†:
- Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ù¾Ø§Ø³Ø®ØŒ ÛŒÚ©â€ŒØ¨Ø§Ø± Ø¨Ú¯Ùˆ: Â«Ù…Ù† Ø¯Ø³ØªÛŒØ§Ø± ManaCare Ù‡Ø³ØªÙ… Ø§Ø² Ú©Ù„ÛŒÙ†ÛŒÚ© DrX.Â»
- Ø¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ ÙÙ‚Ø· Ø¨Ú¯Ùˆ Â«Ù…Ù† Ø¯Ø³ØªÛŒØ§Ø± ManaCare Ù‡Ø³ØªÙ…Â» Ùˆ Ù†Ø§Ù… Ú©Ù„ÛŒÙ†ÛŒÚ© Ø±Ø§ ØªÚ©Ø±Ø§Ø± Ù†Ú©Ù† Ù…Ú¯Ø± Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ.
- Ù‡Ù…ÛŒØ´Ù‡ Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ØŒ Ú¯Ø±Ù…ØŒ Ù‡Ù…Ø¯Ù„Ø§Ù†Ù‡ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ØµØ­Ø¨Øª Ú©Ù†.
- Ø§Ø² Ø¬Ù…Ù„Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ ÙˆØ§Ø¶Ø­ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ù†ÛŒØ¯Ù† ØªÙ„ÙÙ†ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø§Ø´Ù†Ø¯.
- Ø¯Ø± "reply" Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø³Ø§Ø¯Ù‡ Ùˆ Ú©Ø§Ù…Ù„Ø§Ù‹ ÙØ§Ø±Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.

Ù‡Ø¯Ù Ù…Ú©Ø§Ù„Ù…Ù‡:
- Ù†Ø§Ù… Ú©Ø§Ù…Ù„ ØªÙ…Ø§Ø³â€ŒÚ¯ÛŒØ±Ù†Ø¯Ù‡ Ø±Ø§ Ø¨Ú¯ÛŒØ± Ùˆ Ø¯Ø± "name" Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†.
- Ø¢Ø¯Ø±Ø³ Ø±Ø§ Ù‡Ø±Ú†Ù‡ Ø²ÙˆØ¯ØªØ± Ø¨Ú¯ÛŒØ± Ùˆ Ø¯Ø± "address" Ø°Ø®ÛŒØ±Ù‡ Ú©Ù† Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø¨Ù‡â€ŒØµÙˆØ±Øª Ú©ÙˆØªØ§Ù‡ ØªØ£ÛŒÛŒØ¯ Ú©Ù†.
- Ø¯Ù„ÛŒÙ„ Ù…Ø±Ø§Ø¬Ø¹Ù‡ (Ú†Ú©Ø§Ù¾ØŒ Ø¯Ø±Ø¯ Ø®Ø§ØµØŒ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ØŒ Ù…Ø´Ø§ÙˆØ±Ù‡ Ùˆâ€¦) Ø±Ø§ Ø¨Ù¾Ø±Ø³ Ùˆ Ø¯Ø± "notes" Ø«Ø¨Øª Ú©Ù†.
- Ø¨Ø§Ø²Ù‡Ù” Ø²Ù…Ø§Ù†ÛŒ ÛŒØ§ Ø±ÙˆØ² Ùˆ Ø³Ø§Ø¹Øª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ¨Øª Ø±Ø§ Ø¨Ù¾Ø±Ø³ Ùˆ Ø¯Ø± "appointment" Ø«Ø¨Øª Ú©Ù†.
- Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± ØªØ±Ø¬ÛŒØ­Ø§Øª Ø®Ø§ØµÛŒ Ù…Ø«Ù„ Ù¾Ø²Ø´Ú© Ø®Ø§Ù†Ù…/Ø¢Ù‚Ø§ ÛŒØ§ Ø­Ø¶ÙˆØ±ÛŒ/Ø¢Ù†Ù„Ø§ÛŒÙ† Ø¯Ø§Ø±Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø¯Ø± "notes" ÛŒØ§Ø¯Ø¯Ø§Ø´Øª Ú©Ù†.

Ú©Ø§Ø± Ø¨Ø§ Ù†Ø§Ù…â€ŒÙ‡Ø§ (Ø®ÛŒÙ„ÛŒ Ù…Ù‡Ù…):
- Ø±ÙˆÛŒ Ù†Ø§Ù…â€ŒÙ‡Ø§ Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚Øª Ú©Ù† Ùˆ ØªØ§ Ø­Ø¯ Ø§Ù…Ú©Ø§Ù† Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù† (Ù…Ø«Ù„Ø§Ù‹ "Mohammad Reza" â†’ Â«Ù…Ø­Ù…Ø¯Ø±Ø¶Ø§Â»).
- Ø§Ú¯Ø± Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¯Ø± Ø§Ù…Ù„Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø¯Ø± "reply" Ø¨Ø§ ÛŒÚ© Ø³Ø¤Ø§Ù„ Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙˆØ¯Ø¨ Ø§Ù…Ù„Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø±Ø§ ØªØ£ÛŒÛŒØ¯ Ú©Ù†.
- Ø§Ú¯Ø± Ù†Ø§Ù…ÛŒ Ø´Ø¨ÛŒÙ‡ ÛŒÚ©ÛŒ Ø§Ø² Ù…Ø±Ø§Ø¬Ø¹Ø§Ù† Ù‚Ø¨Ù„ÛŒ ÛŒØ§ Â«known clientsÂ» Ú©Ù‡ Ø¯Ø± Ù¾ÛŒØ§Ù… Ø³ÛŒØ³ØªÙ… Ø¢Ù…Ø¯Ù‡ Ø¨Ù‡ Ù†Ø¸Ø± Ø±Ø³ÛŒØ¯ØŒ ÙÙ‚Ø· Ø¨Ø§ Ø§Ø­ØªØ±Ø§Ù… Ø§Ø´Ø§Ø±Ù‡ Ú©Ù† Ú©Ù‡ Â«Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ø§ Ø§ÛŒÙ† Ù†Ø§Ù… Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±ÙˆÙ†Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø±ÛŒÙ…Â» Ùˆ Ø­ØªÙ…Ø§Ù‹ Ø¨Ù¾Ø±Ø³ Ø¢ÛŒØ§ Ø®ÙˆØ¯Ù Ø§ÙˆØ³Øª ÛŒØ§ Ø¨Ø±Ø§ÛŒ Ø´Ø®Øµ Ø¯ÛŒÚ¯Ø±ÛŒ Ù†ÙˆØ¨Øª Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯Ø› Ù‡Ø±Ú¯Ø² Ø®ÙˆØ¯Ø¨Ù‡â€ŒØ®ÙˆØ¯ ÙØ±Ø¶ Ù†Ú©Ù†.

ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ú¯Ø°Ø´ØªÙ‡ (session log):
- Ø§Ø² ØªØ§Ø±ÛŒØ®Ú†Ù‡Ù” Ù…Ú©Ø§Ù„Ù…Ù‡Ù” Ù‡Ù…ÛŒÙ† Ø¬Ù„Ø³Ù‡ Ú©Ù‡ Ø¯Ø± Ù¾ÛŒØ§Ù… Ø³ÛŒØ³ØªÙ… Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù…ØªÙ† Â«Conversation so farÂ» Ù…ÛŒâ€ŒØ¢ÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
- Ø§Ú¯Ø± Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù†Ø§Ù… ÛŒØ§ Ø¢Ø¯Ø±Ø³ ÛŒØ§ ØªØ±Ø¬ÛŒØ­ Ø²Ù…Ø§Ù†ÛŒ Ù‚Ø¨Ù„Ø§Ù‹ Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ØŒ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾Ø±Ø³ÛŒØ¯Ù† Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± JSON Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØŒ Ù…Ú¯Ø± Ø§ÛŒÙ†â€ŒÚ©Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø®ÙˆØ¯Ø´ Ø¢Ù† Ø±Ø§ Ø§ØµÙ„Ø§Ø­ Ú©Ù†Ø¯.
- Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ú†ÛŒØ²ÛŒ Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø²Ù…Ø§Ù† ÛŒØ§ Ø¢Ø¯Ø±Ø³)ØŒ Ù…Ù‚Ø¯Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¯Ø± JSON Ø¨Ù†ÙˆÛŒØ³ Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù†Ú¯ÛŒØ±.

Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ú©Ø§Ù„Ù…Ù‡:
- "reply" Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ÛŒØ¯ Ù…Ø±Ø­Ù„Ù‡Ù” Ø¨Ø¹Ø¯ Ø±Ø§ Ø±ÙˆØ´Ù† Ú©Ù†Ø¯ (Ù…Ø«Ù„Ø§Ù‹: Â«Ø­Ø§Ù„Ø§ Ù„Ø·ÙØ§Ù‹ Ø¢Ø¯Ø±Ø³ Ú©Ø§Ù…Ù„ Ø±Ø§ Ù‡Ù… Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯.Â» ÛŒØ§ Â«Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨ØŒ Ø¨Ø±Ø§ÛŒ Ú†Ù‡ Ù…Ø´Ú©Ù„ÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯ØŸÂ»).
- Ù¾Ø±Ø³Ø´â€ŒÙ‡Ø§ Ø±Ø§ Ø³Ø§Ø¯Ù‡ Ùˆ Ø³Ø±ÛŒØ§Ù„ÛŒ Ù†Ú¯Ù‡ Ø¯Ø§Ø±: Ø§Ø¨ØªØ¯Ø§ Ø³Ù„Ø§Ù… Ùˆ Ù…Ø¹Ø±ÙÛŒØŒ Ø³Ù¾Ø³ Ù†Ø§Ù…ØŒ Ø¨Ø¹Ø¯ Ø¢Ø¯Ø±Ø³ØŒ Ø¨Ø¹Ø¯ Ø¯Ù„ÛŒÙ„ Ù…Ø±Ø§Ø¬Ø¹Ù‡ØŒ Ø¨Ø¹Ø¯ Ø²Ù…Ø§Ù† Ùˆ Ù†ÙˆØ¹ ÙˆÛŒØ²ÛŒØª.
- Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ù…ÙˆØ¶ÙˆØ¹ Ù†Ø§Ù…Ø±ØªØ¨Ø· Ù…Ø·Ø±Ø­ Ú©Ø±Ø¯ØŒ Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ Ø¨Ù‡ Ø§Ùˆ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡ Ú©Ù‡ ØªÙˆ Ø¨Ø±Ø§ÛŒ Ø±Ø²Ø±Ùˆ Ù†ÙˆØ¨Øª Ù‡Ø³ØªÛŒ Ùˆ Ø³Ù¾Ø³ Ø³Ø±ÛŒØ¹Ø§Ù‹ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø±Ø§ Ø¨Ù‡ Ú¯Ø±ÙØªÙ† Ø²Ù…Ø§Ù†/Ø¯Ù„ÛŒÙ„ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†.

Ù¾Ø§ÛŒØ§Ù† Ù…Ú©Ø§Ù„Ù…Ù‡:
- ÙˆÙ‚ØªÛŒ Ù†Ø§Ù…ØŒ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø³Ø·Ø­ Ø§Ø² Ø¢Ø¯Ø±Ø³ØŒ Ø¯Ù„ÛŒÙ„ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ùˆ ØªØ±Ø¬ÛŒØ­ Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ Ø¯Ø§Ù†Ø³ØªÛŒØŒ Ø¯Ø± "reply" Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡ Ø§Ø² Ø¢Ù†Ú†Ù‡ Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø¨Ú¯Ùˆ.
- Ø³Ù¾Ø³ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† Ú©Ù‡ Â«ØªÛŒÙ… ManaCare Ø¯Ø± Ú©Ù„ÛŒÙ†ÛŒÚ© DrX ØªØ£ÛŒÛŒØ¯ Ù†Ù‡Ø§ÛŒÛŒ Ù†ÙˆØ¨Øª Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§ Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.Â»

Ù†Ù…ÙˆÙ†Ù‡Ù” JSON Ø®Ø±ÙˆØ¬ÛŒ:
{"reply":"Ø³Ù„Ø§Ù…ØŒ Ù…Ù† Ø¯Ø³ØªÛŒØ§Ø± ManaCare Ù‡Ø³ØªÙ… Ø§Ø² Ú©Ù„ÛŒÙ†ÛŒÚ© DrX. Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ú©Ø§Ù…Ù„ Ø´Ù…Ø§ Ø±Ø§ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯.","name":null,"address":null,"appointment":null,"notes":null}

Ø¨Ù‡â€ŒÛŒØ§Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´: Ù‡Ù…ÛŒØ´Ù‡ ÙÙ‚Ø· ÛŒÚ© Ø´ÛŒØ¡ JSON Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ØŒ Ø¨Ø¯ÙˆÙ† Ù…ØªÙ† Ø§Ø¶Ø§ÙÛŒ.
""".strip()


@dataclass
class BotConfig:
    sample_rate: int = 16000
    record_seconds: float = 8.0

    push_chunk_seconds: float = 0.3
    push_silence_timeout: float = 0.6
    push_energy_threshold: float = 80.0
    silence_trim_threshold: float = 40.0

    realtime_chunk_seconds: float = 0.25
    realtime_silence_timeout: float = 0.35
    realtime_energy_threshold: float = 200.0

    vad_aggressiveness: int = 1
    use_vad_for_filtering: bool = False  # Ø§Ú¯Ø± Ø®ÙˆØ§Ø³ØªÛŒ ÙÙ‚Ø· speech-frameÙ‡Ø§ ØªØ±Ù†Ø³Ú©Ø±Ø§ÛŒØ¨ Ø´ÙˆÙ†Ø¯ØŒ True Ú©Ù†.

    history_limit: int = 16

    tts_model: str = os.getenv("OPENAI_TTS_MODEL", "gpt-4o-mini-tts")
    tts_voice: str = os.getenv("OPENAI_TTS_VOICE", "alloy")
    response_model: str = os.getenv("OPENAI_RESPONSE_MODEL", "gpt-4o-mini")
    transcription_model: str = os.getenv(
        "OPENAI_TRANSCRIBE_MODEL", "gpt-4o-mini-transcribe"
    )
    transcription_fallback: str = os.getenv(
        "OPENAI_TRANSCRIBE_FALLBACK", "gpt-4o-mini-transcribe"
    )

    data_dir: Path = Path("data")


class VoiceDoctorBot:
    """Conversational loop that records audio, transcribes, reasons, and speaks."""

    def __init__(self, config: Optional[BotConfig] = None) -> None:
        load_dotenv()
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("Missing OPENAI_API_KEY in environment or .env file.")

        self.config = config or BotConfig()
        self.client = OpenAI(api_key=api_key)

        self.is_speaking = False
        self.sample_rate = self.config.sample_rate
        self.record_seconds = self.config.record_seconds

        # state
        self.profile: Dict[str, Optional[str]] = {"name": None, "address": None}
        self.notes: List[str] = []
        self.previous_snapshot: Optional[Dict[str, Any]] = None
        self.history: List[Dict[str, str]] = []
        self._transcribe_warned = False

        # files
        self.data_dir = self.config.data_dir
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.session_log = self.data_dir / "session_log.txt"
        self.profile_file = self.data_dir / "last_session.json"
        self.clients_file = self.data_dir / "clients.json"
        self.session_meta_file = self.data_dir / "session_meta.json"
        self.prompt_file = self.data_dir / "custom_prompt.txt"
        self.names_file = self.data_dir / "iranian_names.txt"

        # tts
        self.tts_engine = pyttsx3.init()
        self.tts_model = self.config.tts_model
        self.tts_voice = self.config.tts_voice

        # models
        self.response_model = self.config.response_model
        self.transcription_model = self.config.transcription_model
        self.transcription_fallback = self.config.transcription_fallback

        # misc config
        self.push_chunk_seconds = self.config.push_chunk_seconds
        self.push_silence_timeout = self.config.push_silence_timeout
        self.push_energy_threshold = self.config.push_energy_threshold
        self.silence_trim_threshold = self.config.silence_trim_threshold
        self.history_limit = self.config.history_limit

        # session
        self.session_name = self._generate_session_name()
        self.known_clients: set[str] = self._load_known_clients()
        self.iranian_name_list: set[str] = self._load_iranian_names()
        self.system_prompt = self._load_system_prompt()
        self._load_last_session()
        self._load_history_from_log()
        self._select_persian_voice()
        self._save_session_meta()
        self._log_session_start()

        # VAD
        self.vad = webrtcvad.Vad(self.config.vad_aggressiveness)

    # ---------- Public entrypoints ----------

    def run(self) -> None:
        print(
            f"Doctor Voice Assistant ready. Session '{self.session_name}'. "
            "Press Enter to speak, 'q' to quit."
        )
        while True:
            user_input = input("Press Enter to speak (q to quit): ").strip().lower()
            if user_input == "q":
                print("Session ended. See data/last_session.json for captured details.")
                break

            try:
                audio_np = self._record_audio()
                if audio_np.size == 0:
                    print("No audio captured. Try again.")
                    continue
                transcript = self._transcribe(audio_np)
            except Exception as exc:  # broad to keep loop alive
                print(f"Recording or transcription failed: {exc}")
                continue

            if not transcript:
                print("No speech detected. Try again.")
                continue

            print(f"You said: {transcript}")
            self._log("user", transcript)

            reply, payload = self._reason(transcript)
            if not reply:
                print("The assistant could not create a response. Try again.")
                continue

            print(f"Assistant: {reply}")
            self._speak(reply)
            self._log("assistant", reply)
            self._update_profile(payload)

    def train_prompt(self) -> None:
        print(
            "Prompt training mode.\n"
            f"Speak for up to {self.record_seconds} seconds to describe how the assistant should behave."
        )
        try:
            audio_np = self._record_audio()
            transcript = self._transcribe(audio_np)
        except Exception as exc:
            print(f"Training failed: {exc}")
            return
        if not transcript:
            print("Did not capture any speech. Prompt unchanged.")
            return
        print(f"Captured prompt:\n{transcript}")
        self._save_system_prompt(transcript)
        print(f"Custom prompt saved to {self.prompt_file}.")

    def run_realtime(
        self,
        chunk_seconds: Optional[float] = None,
        silence_timeout: Optional[float] = None,
        energy_threshold: Optional[float] = None,
    ) -> None:
        """Continuously listen for speech and answer as soon as silence is detected."""
        chunk_seconds = chunk_seconds or self.config.realtime_chunk_seconds
        silence_timeout = silence_timeout or self.config.realtime_silence_timeout
        energy_threshold = energy_threshold or self.config.realtime_energy_threshold

        print(
            "Realtime Doctor Voice Assistant listening.\n"
            f"Session '{self.session_name}' at DrX clinic.\n"
            "Speak naturally; pause for a second to let the assistant reply.\n"
            "Press Ctrl+C to end the session."
        )
        chunk_frames = int(self.sample_rate * chunk_seconds)
        audio_queue: "queue.Queue[np.ndarray]" = queue.Queue()

        def _callback(indata, frames, time_info, status):
            if status:
                print(f"Audio warning: {status}")
            if self.is_speaking:
                return  # discard mic frames while assistant voice is playing
            audio_queue.put(indata.copy().flatten())

        buffer: List[np.ndarray] = []
        silence_since: Optional[float] = None

        try:
            with sd.InputStream(
                samplerate=self.sample_rate,
                channels=1,
                dtype="int16",
                blocksize=chunk_frames,
                callback=_callback,
            ):
                while True:
                    chunk = audio_queue.get()
                    energy = float(np.mean(np.abs(chunk)))
                    if energy >= energy_threshold:
                        buffer.append(chunk)
                        silence_since = None
                    elif buffer:
                        now = time.time()
                        silence_since = silence_since or now
                        if (now - silence_since) >= silence_timeout:
                            segment = np.concatenate(buffer)
                            buffer.clear()
                            silence_since = None
                            self._process_segment(segment)
        except KeyboardInterrupt:
            print("\nSession ended. See data/last_session.json for captured details.")

    # ---------- Audio capture & processing ----------

    def _record_audio(self) -> np.ndarray:
        print(
            f"Recording (max {self.record_seconds:.1f}s)... "
            "stop speaking to send sooner."
        )
        sd.stop()
        chunk_frames = max(1, int(self.sample_rate * self.push_chunk_seconds))
        max_frames = int(self.sample_rate * self.record_seconds)
        audio_queue: "queue.Queue[np.ndarray]" = queue.Queue()

        def _callback(indata, frames, time_info, status) -> None:  # type: ignore[override]
            if status:
                print(f"Audio warning: {status}")
            audio_queue.put(indata.copy().flatten())

        buffer: List[np.ndarray] = []
        total_frames = 0
        silence_since: Optional[float] = None

        try:
            with sd.InputStream(
                samplerate=self.sample_rate,
                channels=1,
                dtype="int16",
                blocksize=chunk_frames,
                callback=_callback,
            ):
                while True:
                    chunk = audio_queue.get()
                    buffer.append(chunk)
                    total_frames += len(chunk)
                    energy = float(np.mean(np.abs(chunk)))
                    now = time.time()
                    if energy >= self.push_energy_threshold:
                        silence_since = None
                    else:
                        silence_since = silence_since or now
                        elapsed_silence = now - silence_since
                        min_duration = 0.5
                        if (
                            elapsed_silence >= self.push_silence_timeout
                            and total_frames >= int(self.sample_rate * min_duration)
                        ):
                            break
                    if total_frames >= max_frames:
                        break
        except Exception as exc:
            print(f"Streaming record failed ({exc}); falling back to fixed window.")
            frames = max_frames
            audio = sd.rec(frames, samplerate=self.sample_rate, channels=1, dtype="int16")
            sd.wait()
            return audio.flatten()

        if not buffer:
            return np.array([], dtype=np.int16)

        audio = np.concatenate(buffer)

        # debug save
        debug_file = self.data_dir / "last_raw.wav"
        try:
            with wave.open(str(debug_file), "wb") as f:
                f.setnchannels(1)
                f.setsampwidth(2)
                f.setframerate(self.sample_rate)
                f.writeframes(audio.tobytes())
            print(f"[debug] saved raw audio to {debug_file}")
        except Exception as exc:
            print(f"[debug] failed to save raw audio: {exc}")

        return self._trim_trailing_silence(audio)

    def _process_segment(self, audio_np: np.ndarray) -> None:
        """Realtime: trim, optionally VAD-filter, transcribe, reason, speak."""
        if audio_np.size == 0:
            return

        audio_np = self._trim_trailing_silence(audio_np)

        # Ø§Ø®ØªÛŒØ§Ø±ÛŒ: ÙÙ‚Ø· Ù‚Ø·Ø¹Ø§Øª Ø­Ø§ÙˆÛŒ Ú¯ÙØªØ§Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ…
        if self.config.use_vad_for_filtering:
            speech_only = self._keep_speech_only(audio_np)
            if speech_only.size > 0:
                audio_np = speech_only

        if audio_np.size == 0:
            print("[audio] no usable speech after filtering.")
            return

        transcript = self._transcribe(audio_np)
        if not transcript:
            print("No transcript from model.")
            return

        print(f"You said: {transcript}")
        self._log("user", transcript)

        reply, payload = self._reason(transcript)
        if not reply:
            print("The assistant could not create a response. Try again.")
            return

        print(f"Assistant: {reply}")
        self._speak(reply)
        self._log("assistant", reply)
        self._update_profile(payload)

    def _trim_trailing_silence(self, audio: np.ndarray) -> np.ndarray:
        if audio.size == 0:
            return audio
        threshold = self.silence_trim_threshold
        abs_audio = np.abs(audio)
        idx = len(audio) - 1
        while idx >= 0 and abs_audio[idx] <= threshold:
            idx -= 1
        if idx <= 0:
            return audio
        return audio[: idx + 1]

    def _keep_speech_only(self, audio: np.ndarray, frame_ms: int = 20) -> np.ndarray:
        if audio.size == 0:
            return audio

        if audio.dtype != np.int16:
            audio = audio.astype(np.int16)

        sample_rate = self.sample_rate
        frame_len = int(sample_rate * frame_ms / 1000)
        raw = audio.tobytes()

        speech_bytes = bytearray()
        for offset in range(0, len(raw), frame_len * 2):
            chunk = raw[offset: offset + frame_len * 2]
            if len(chunk) < frame_len * 2:
                break
            if self.vad.is_speech(chunk, sample_rate):
                speech_bytes.extend(chunk)

        if not speech_bytes:
            return np.array([], dtype=np.int16)

        return np.frombuffer(bytes(speech_bytes), dtype=np.int16)

    # ---------- Transcription ----------

    def _to_wav_bytes(self, audio: np.ndarray) -> io.BytesIO:
        buffer = io.BytesIO()
        with wave.open(buffer, "wb") as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(self.sample_rate)
            wav_file.writeframes(audio.tobytes())
        buffer.seek(0)
        return buffer

    def _build_name_prompt(self) -> str:
        known = sorted(self.known_clients)
        tail = known[-10:]
        base = (
            "Ø§ÛŒÙ† ØªÙ…Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ø±Ø²Ø±Ùˆ Ù†ÙˆØ¨Øª Ø§Ø³Øª. Ù†Ø§Ù… Ùˆ Ù†Ø§Ù…â€ŒØ®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ ÙØ§Ø±Ø³ÛŒ Ù…Ø±Ø§Ø¬Ø¹Ù‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚ Ø¨Ù†ÙˆÛŒØ³. "
            "Ø§Ú¯Ø± Ø¯Ø± ÙØ§ÛŒÙ„ ÙÙ‚Ø· Ù…ÙˆØ³ÛŒÙ‚ÛŒØŒ Ù†ÙˆÛŒØ² ÛŒØ§ ØµØ¯Ø§Ù‡Ø§ÛŒ Ù…Ø¨Ù‡Ù… Ø´Ù†ÛŒØ¯ÛŒ Ùˆ Ú¯ÙØªØ§Ø± ÙˆØ§Ø¶Ø­ ÙØ§Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ "
            "Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø®Ø§Ù„ÛŒ Ø¨Ú¯Ø°Ø§Ø± Ùˆ Ù‡ÛŒÚ† Ù…ØªÙ†ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù†Ú©Ù†."
        )
        if not tail:
            return base
        joined = "ØŒ ".join(tail)
        return base + f" Ø¨Ø±Ø®ÛŒ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ: {joined}."

    def _transcribe(self, audio: np.ndarray) -> str:
        audio_buffer = self._to_wav_bytes(audio)
        audio_buffer.name = "speech.wav"

        use_audio_endpoint = any(
            marker in self.transcription_model.lower()
            for marker in ("transcribe", "whisper")
        )
        if use_audio_endpoint:
            return self._transcribe_via_audio_endpoint(audio_buffer, self.transcription_model)

        try:
            return self._transcribe_via_responses(audio_buffer)
        except OpenAIError as exc:
            if not self._transcribe_warned:
                print(
                    f"Advanced transcription with '{self.transcription_model}' failed "
                    f"({exc}). Falling back to '{self.transcription_fallback}'."
                )
                self._transcribe_warned = True
            return self._transcribe_via_audio_endpoint(audio_buffer, self.transcription_fallback)

    def _transcribe_via_audio_endpoint(self, audio_buffer: io.BytesIO, model: str) -> str:
        clone = io.BytesIO(audio_buffer.getvalue())
        clone.name = "speech.wav"
        result = self.client.audio.transcriptions.create(
            model=model,
            file=clone,
            language="fa",
            prompt=self._build_name_prompt(),
        )
        return (result.text or "").strip()

    def _transcribe_via_responses(self, audio_buffer: io.BytesIO) -> str:
        payload = base64.b64encode(audio_buffer.getvalue()).decode("ascii")
        instruction = (
            "Transcribe the following Persian speech. The audio is a base64-encoded WAV "
            "string. Decode it and reply with only the transcript.\n"
            f"{payload}"
        )
        response = self.client.responses.create(
            model=self.transcription_model,
            input=[
                {
                    "role": "user",
                    "content": [{"type": "input_text", "text": instruction}],
                }
            ],
        )
        return self._extract_text(response)

    def _extract_text(self, response) -> str:
        chunks: List[str] = []
        for item in getattr(response, "output", []) or []:
            for content in getattr(item, "content", []) or []:
                if getattr(content, "type", None) == "output_text":
                    chunks.append(getattr(content, "text", ""))
        return "".join(chunks).strip()

    # ---------- Reasoning / JSON orchestration ----------

    def _is_pure_test_utterance(self, transcript: str) -> bool:
        """
        Heuristic: detect when user is clearly just testing audio,
        not really booking an appointment.
        """
        txt = transcript.replace("ðŸŽ¤", "").strip().lower()

        test_keywords = [
            "ØªØ³Øª ØµØ¯Ø§",
            "ØªØ³Øª Ø¶Ø¨Ø·",
            "Ø¢Ø²Ù…Ø§ÛŒØ´ ØµØ¯Ø§",
            "Ø¢Ø²Ù…Ø§ÛŒØ´ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†",
            "Ø¢Ø²Ù…Ø§ÛŒØ´ Ù…ÛŒÚ©Ø±ÙˆÙÙ†",
            "Ú©Ø§Ù„ÛŒØ¨Ø±Ù‡",
            "Ú©Ø§Ù„ÛŒØ¨Ø±Ù‡â€ŒÚ©Ø±Ø¯Ù†",
            "Ø¨Ø±Ø§ÛŒ ØªØ³Øª",
            "ÙÙ‚Ø· ØªØ³Øª",
            "ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´",
        ]

        booking_keywords = [
            "Ù†ÙˆØ¨Øª",
            "ÙˆÛŒØ²ÛŒØª",
            "ÙˆÙŠØ²ÙŠØª",
            "ÙˆÙ‚Øª",
            "Ù…Ø´Ø§ÙˆØ±Ù‡",
            "Ø¯Ú©ØªØ±",
            "Ø¯ÙƒØªØ±",
            "Ù¾Ø²Ø´Ú©",
            "Ú©Ù„ÛŒÙ†ÛŒÚ©",
            "ÙƒÙ„ÙŠÙ†ÙŠÙƒ",
        ]

        if any(k in txt for k in booking_keywords):
            return False
        return any(k in txt for k in test_keywords)

    def _reason(self, transcript: str) -> Tuple[str, Dict[str, Optional[str]]]:
        # ØªØ³Øª Ø³Ø§Ø¯Ù‡ Ø¨Ø¯ÙˆÙ† Ø±ÙØªÙ† Ø¨Ù‡ LLM
        if self._is_pure_test_utterance(transcript):
            payload: Dict[str, Optional[str]] = {
                "intent": "test",
                "reply": (
                    "Ù…Ù† Ø¯Ø³ØªÛŒØ§Ø± ManaCare Ù‡Ø³ØªÙ…. Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ØªØ³Øª ØµØ¯Ø§ Ø«Ø¨Øª Ø´Ø¯Ø› "
                    "Ù‡Ø± Ø²Ù…Ø§Ù† Ø¢Ù…Ø§Ø¯Ù‡ Ù†ÙˆØ¨Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¨ÙˆØ¯ÛŒØ¯ØŒ Ù†Ø§Ù… Ùˆ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒØªØ§Ù† Ø±Ø§ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯."
                ),
                "name": self.profile.get("name"),
                "address": self.profile.get("address"),
                "appointment": None,
                "notes": None,
            }
            return payload["reply"], payload

        profile_json = json.dumps(
            {"name": self.profile.get("name"), "address": self.profile.get("address")},
            ensure_ascii=False,
        )
        history_context = self._history_context()
        previous_snapshot_json = json.dumps(self.previous_snapshot or {}, ensure_ascii=False)
        known_clients_list = sorted(self.known_clients)
        client_json = json.dumps(known_clients_list[-20:], ensure_ascii=False)
        possible_return = self._find_similar_client(transcript) or "none"

        prompt = (
            f"Session name: {self.session_name}\n"
            f"Known returning clients: {client_json}\n"
            f"Possible returning client mentioned: {possible_return}\n"
            "Previous session snapshot (for reference onlyâ€”confirm before reuse): "
            f"{previous_snapshot_json}\n"
            "Conversation so far:\n"
            f"{history_context}\n"
            f"Caller statement: {transcript}\n"
            f"Known data: {profile_json}\n"
            "ÙˆØ¸ÛŒÙÙ‡ ØªÙˆ:\n"
            "- ÙÛŒÙ„Ø¯ \"intent\" Ø±Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù‚Ø±Ø§Ø± Ø¨Ø¯Ù‡: "
            "\"booking\" (Ø±Ø²Ø±Ùˆ Ù†ÙˆØ¨Øª)ØŒ \"test\" (ØªØ³Øª ØµØ¯Ø§/Ø³ÛŒØ³ØªÙ…)ØŒ "
            "\"noise\" (Ù†ÙˆÛŒØ² ÛŒØ§ Ù…Ø­ØªÙˆØ§ÛŒ Ù†Ø§Ù…Ø±Ø¨ÙˆØ·)ØŒ \"other\" (Ø³Ø§ÛŒØ± Ù…ÙˆØ§Ø±Ø¯).\n"
            "- ÙÙ‚Ø· Ø§Ú¯Ø± intent = \"booking\" Ø¨ÙˆØ¯ Ù†Ø§Ù…ØŒ Ø¢Ø¯Ø±Ø³ØŒ Ù†ÙˆØ¨Øª Ùˆ notes Ø±Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø¬Ø¯ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ² Ú©Ù†.\n"
            "- Ø§Ú¯Ø± intent Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ \"test\" ÛŒØ§ \"noise\" Ø¨ÙˆØ¯ØŒ name Ùˆ address Ùˆ appointment Ø±Ø§ ØªØºÛŒÛŒØ± Ù†Ø¯Ù‡ Ùˆ ÙÙ‚Ø· ÛŒÚ© reply Ù…ÙˆØ¯Ø¨ Ø¨Ø¯Ù‡.\n"
            "- Ù‡Ù…ÛŒØ´Ù‡ ÙÙ‚Ø· ÛŒÚ© JSON Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù† Ù…Ø«Ù„:\n"
            "{\"intent\":\"booking\",\"reply\":\"...\",\"name\":null,\"address\":null,\"appointment\":null,\"notes\":null}\n"
        )

        try:
            response = self.client.responses.create(
                model=self.response_model,
                temperature=0.1,
                input=[
                    {
                        "role": "system",
                        "content": [{"type": "input_text", "text": self.system_prompt}],
                    },
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": prompt}],
                    },
                ],
            )
        except OpenAIError as exc:
            print(f"OpenAI request failed: {exc}")
            # fallback: Ø¬ÙˆØ§Ø¨ Ø®ÛŒÙ„ÛŒ Ø³Ø§Ø¯Ù‡
            fallback_reply = (
                "Ù…Ù† Ø¯Ø³ØªÛŒØ§Ø± ManaCare Ù‡Ø³ØªÙ…. Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® ÙÙ†ÛŒ Ù…Ø´Ú©Ù„ Ù¾ÛŒØ´ Ø¢Ù…Ø¯Ø› "
                "Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø¨Ø§Ø± Ø¯ÛŒÚ¯Ø± Ø¨Ù‡â€ŒØµÙˆØ±Øª Ú©ÙˆØªØ§Ù‡ Ù†Ø§Ù… Ùˆ Ø¯Ù„ÛŒÙ„ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø±Ø§ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯."
            )
            payload = {
                "intent": "other",
                "reply": fallback_reply,
                "name": self.profile.get("name"),
                "address": self.profile.get("address"),
                "appointment": None,
                "notes": None,
            }
            return fallback_reply, payload

        raw_text = self._extract_text(response)
        payload = self._normalize_payload(raw_text)

        intent = payload.get("intent")

        # clamp behavior for non-booking
        if intent in ("test", "noise"):
            payload["reply"] = (
                "Ù…Ù† Ø¯Ø³ØªÛŒØ§Ø± ManaCare Ù‡Ø³ØªÙ…. ØµØ¯Ø§ÛŒ Ø´Ù…Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù…Ø› "
                "Ù‡Ø± Ø²Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ¨Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ÛŒØ¯ØŒ ÙÙ‚Ø· Ù†Ø§Ù… Ùˆ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒØªØ§Ù† Ø±Ø§ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯."
                if intent == "test"
                else "Ù…Ù† Ø¯Ø³ØªÛŒØ§Ø± ManaCare Ù‡Ø³ØªÙ…. Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ ØµØ¯Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø±Ø²Ø±Ùˆ Ù†ÙˆØ¨Øª Ø¯Ø±ÛŒØ§ÙØª Ù†Ú©Ø±Ø¯Ù…Ø› "
                     "Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ ÙˆÙ‚Øª Ø¨Ú¯ÛŒØ±ÛŒØ¯ØŒ Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ùˆ Ø¯Ù„ÛŒÙ„ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø±Ø§ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯."
            )
            payload["appointment"] = None
            payload["notes"] = None

        reply = payload.get("reply") or raw_text
        return reply, payload

    def _normalize_payload(self, blob: str) -> Dict[str, Optional[str]]:
        """
        Ensure we always return a dict with standard keys:
        intent, reply, name, address, appointment, notes
        """
        data: Dict[str, Any]
        try:
            data_raw = json.loads(blob)
            if isinstance(data_raw, dict):
                data = data_raw
            else:
                data = {}
        except json.JSONDecodeError:
            data = {}

        intent = str(data.get("intent") or "booking")
        reply = data.get("reply")
        name = data.get("name")
        address = data.get("address")
        appointment = data.get("appointment")
        notes = data.get("notes")

        # normalize nulls
        def _clean(x):
            if x is None:
                return None
            s = str(x).strip()
            return s or None

        return {
            "intent": _clean(intent),
            "reply": _clean(reply),
            "name": _clean(name),
            "address": _clean(address),
            "appointment": _clean(appointment),
            "notes": _clean(notes),
        }

    # ---------- TTS ----------

    def _select_persian_voice(self) -> None:
        """Pick a Persian-capable TTS voice if the system has one."""
        try:
            voices = self.tts_engine.getProperty("voices")
        except Exception:
            return

        def _match(voice, keyword: str) -> bool:
            name = (getattr(voice, "name", "") or "").lower()
            langs = ",".join(
                str(lang).lower() for lang in getattr(voice, "languages", []) or []
            )
            return keyword in name or keyword in langs

        for keyword in ("persian", "farsi", "iran"):
            for voice in voices:
                if _match(voice, keyword):
                    self.tts_engine.setProperty("voice", voice.id)
                    return

        for voice in voices:
            if _match(voice, "fa"):
                self.tts_engine.setProperty("voice", voice.id)
                return

    def _synthesize_with_openai(self, message: str) -> bytes:
        response = self.client.audio.speech.create(
            model=self.tts_model,
            voice=self.tts_voice,
            input=message,
            response_format="wav",
            instructions="Speak naturally in Persian.",
        )
        return response.read()

    def _play_wav_bytes(self, wav_bytes: bytes) -> None:
        if not wav_bytes:
            return
        sd.stop()
        buffer = io.BytesIO(wav_bytes)
        with wave.open(buffer, "rb") as wav_file:
            sample_rate = wav_file.getframerate()
            channels = wav_file.getnchannels()
            sample_width = wav_file.getsampwidth()
            frames = wav_file.readframes(wav_file.getnframes())

        if sample_width == 1:
            audio = np.frombuffer(frames, dtype=np.uint8).astype(np.float32)
            audio = (audio - 128) / 128.0
        elif sample_width == 2:
            audio = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
        elif sample_width == 4:
            audio = np.frombuffer(frames, dtype=np.int32).astype(np.float32) / 2147483648.0
        else:
            raise ValueError(f"Unsupported sample width: {sample_width}")

        if channels > 1:
            audio = audio.reshape(-1, channels)

        sd.play(audio, sample_rate)
        sd.wait()

    def _speak(self, message: str) -> None:
        if not message:
            return
        self.is_speaking = True
        try:
            audio_bytes = self._synthesize_with_openai(message)
            self._play_wav_bytes(audio_bytes)
        finally:
            self.is_speaking = False

    # ---------- Session / persistence ----------

    def _generate_session_name(self) -> str:
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        suffix = os.urandom(2).hex()
        return f"drx-{timestamp}-{suffix}"

    def _save_session_meta(self) -> None:
        meta = {
            "session_name": self.session_name,
            "started_at": time.strftime("%Y-%m-%dT%H:%M:%S"),
        }
        self.session_meta_file.write_text(json.dumps(meta, indent=2), encoding="utf-8")

    def _log_session_start(self) -> None:
        line = f"[{self.session_name}] session: started at {time.ctime()}\n"
        with self.session_log.open("a", encoding="utf-8") as handle:
            handle.write(line)

    def _load_last_session(self) -> None:
        """Seed profile and notes from the previous session if available."""
        self.previous_snapshot = None
        if not self.profile_file.exists():
            return
        try:
            data = json.loads(self.profile_file.read_text())
        except Exception:
            return
        if isinstance(data, dict):
            self.previous_snapshot = data

    def _load_system_prompt(self) -> str:
        if self.prompt_file.exists():
            try:
                text = self.prompt_file.read_text(encoding="utf-8").strip()
                if text:
                    return text
            except Exception:
                pass
        return SYSTEM_PROMPT

    def _save_system_prompt(self, prompt: str) -> None:
        content = prompt.strip() or SYSTEM_PROMPT
        self.prompt_file.write_text(content, encoding="utf-8")
        self.system_prompt = content

    def _load_history_from_log(self) -> None:
        entries: List[Dict[str, str]] = []
        for _, role, text in self._iter_log_entries():
            if role in ("user", "assistant"):
                entries.append({"role": role, "text": text})
        if entries:
            self.history = entries[-self.history_limit :]

    def _load_known_clients(self) -> set[str]:
        if self.clients_file.exists():
            try:
                data = json.loads(self.clients_file.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    return {str(item).strip() for item in data if str(item).strip()}
            except Exception:
                pass
        names: set[str] = set()
        for _, role, text in self._iter_log_entries():
            if role != "assistant":
                continue
            payload = self._parse_json(text)
            name = payload.get("name")
            if isinstance(name, str) and name.strip():
                names.add(name.strip())
        return names

    def _load_iranian_names(self) -> set[str]:
        """
        Load a set of common Iranian first names.
        """
        if self.names_file.exists():
            try:
                raw = self.names_file.read_text(encoding="utf-8")
                tokens = re.split(r"[\n,;]+", raw)
                names: set[str] = set()
                for t in tokens:
                    t = t.strip()
                    if not t:
                        continue
                    if re.search(r"[A-Za-z]", t):
                        continue
                    names.add(_normalize_persian_name(t))
                if names:
                    return names
            except Exception:
                pass
        return {_normalize_persian_name(n) for n in IRANIAN_DEFAULT_NAMES}

    def _persist_known_clients(self) -> None:
        sorted_names = sorted(self.known_clients)
        self.clients_file.write_text(
            json.dumps(sorted_names, indent=2, ensure_ascii=False), encoding="utf-8"
        )

    def _add_known_client(self, name: str) -> None:
        clean = name.strip()
        if not clean:
            return
        if clean not in self.known_clients:
            self.known_clients.add(clean)
            self._persist_known_clients()

    def _parse_json(self, blob: str) -> Dict[str, Optional[str]]:
        try:
            data = json.loads(blob)
            if isinstance(data, dict):
                return data
        except json.JSONDecodeError:
            pass
        return {"reply": blob}

    def _update_profile(self, payload: Dict[str, Optional[str]]) -> None:
        updated = False

        raw_name = payload.get("name")
        if isinstance(raw_name, str) and raw_name.strip():
            name = _normalize_persian_name(raw_name)
            if self.profile.get("name") != name:
                self.profile["name"] = name
                updated = True
            self._add_known_client(name)

        raw_addr = payload.get("address")
        if isinstance(raw_addr, str) and raw_addr.strip():
            addr = raw_addr.strip()
            if self.profile.get("address") != addr:
                self.profile["address"] = addr
                updated = True

        note = payload.get("notes") or payload.get("appointment")
        if note:
            self.notes.append(str(note))
            updated = True

        if updated:
            snapshot = {
                "session": self.session_name,
                "profile": self.profile,
                "notes": self.notes,
            }
            self.profile_file.write_text(
                json.dumps(snapshot, indent=2, ensure_ascii=False),
                encoding="utf-8",
            )
            self.previous_snapshot = snapshot
            print("Profile updated:", snapshot)

    # ---------- History / logging ----------

    def _log(self, role: str, text: str) -> None:
        line = f"[{self.session_name}] {role}: {text}\n"
        with self.session_log.open("a", encoding="utf-8") as handle:
            handle.write(line)
        self._remember(role, text)

    def _remember(self, role: str, text: str) -> None:
        self.history.append({"role": role, "text": text})
        if len(self.history) > self.history_limit:
            self.history = self.history[-self.history_limit :]

    def _history_context(self) -> str:
        if not self.history:
            return "No prior conversation."
        return "\n".join(f"{item['role']}: {item['text']}" for item in self.history)

    def _find_similar_client(self, transcript: str) -> Optional[str]:
        lower_transcript = transcript.lower()
        for name in sorted(self.known_clients):
            normalized = name.lower()
            if normalized and normalized in lower_transcript:
                return name
        return None

    def _parse_log_line(self, line: str):
        stripped = line.rstrip("\n")
        if not stripped:
            return None
        session = None
        remainder = stripped
        if stripped.startswith("["):
            closing = stripped.find("]")
            if closing != -1:
                session = stripped[1:closing]
                remainder = stripped[closing + 1 :].lstrip()
        if ": " not in remainder:
            return None
        role, text = remainder.split(": ", 1)
        return session, role.strip(), text

    def _iter_log_entries(self):
        if not self.session_log.exists():
            return
        current_session: Optional[str] = None
        current_role: Optional[str] = None
        current_lines: List[str] = []
        with self.session_log.open("r", encoding="utf-8") as handle:
            for raw_line in handle:
                parsed = self._parse_log_line(raw_line)
                if parsed:
                    if current_role:
                        yield (
                            current_session,
                            current_role,
                            "\n".join(current_lines).strip(),
                        )
                    current_session, current_role, text = parsed
                    current_lines = [text]
                else:
                    if current_role is not None:
                        current_lines.append(raw_line.rstrip("\n"))
        if current_role:
            yield current_session, current_role, "\n".join(current_lines).strip()


# ---------- CLI ----------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Doctor voice assistant powered by OpenAI (refactored)."
    )
    parser.add_argument(
        "--realtime",
        action="store_true",
        help="Continuously listen with automatic speech detection.",
    )
    parser.add_argument(
        "--chunk-seconds",
        type=float,
        default=None,
        help="Realtime chunk size (seconds) for faster speech detection.",
    )
    parser.add_argument(
        "--silence-timeout",
        type=float,
        default=None,
        help="Silence duration (seconds) that triggers a response in realtime mode.",
    )
    parser.add_argument(
        "--energy-threshold",
        type=float,
        default=None,
        help="Minimum average energy to treat audio as speech in realtime mode.",
    )
    parser.add_argument(
        "--record-seconds",
        type=float,
        default=None,
        help="Max duration for push-to-talk recordings.",
    )
    parser.add_argument(
        "--train-prompt",
        action="store_true",
        help="Capture a new system prompt from voice input and exit.",
    )
    args = parser.parse_args()

    cfg = BotConfig()
    if args.record_seconds is not None:
        cfg.record_seconds = args.record_seconds

    bot = VoiceDoctorBot(config=cfg)

    if args.train_prompt:
        bot.train_prompt()
    elif args.realtime:
        bot.run_realtime(
            chunk_seconds=args.chunk_seconds,
            silence_timeout=args.silence_timeout,
            energy_threshold=args.energy_threshold,
        )
    else:
        bot.run()
